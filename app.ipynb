{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def newfile(file):\n",
    "  with open(file, 'w') as fp:\n",
    "    fp.writelines('{\"data\":[]}')\n",
    "    pass\n",
    "\n",
    "def write_json(new_data,file):\n",
    "  with open(file, 'r+') as file:\n",
    "    file_data = json.load(file)\n",
    "    file_data['data'].append(new_data)\n",
    "    file.seek(0)\n",
    "    json.dump(file_data, file, indent=4)\n",
    "\n",
    "\n",
    "def products_scraper(file):\n",
    "  baseurl = 'https://www.bplmedicaltechnologies.com'\n",
    "  url1 = '/products/'\n",
    "  r = requests.get(url=baseurl+url1)\n",
    "  data = r.content\n",
    "  products = []\n",
    "  soup = BeautifulSoup(data, \"html.parser\")\n",
    "  productsoup = soup.find('ul', {'class': \"prd-lst-ul contentscroll2\"})\n",
    "  for product in productsoup.find_all('div', {'class': \"white-box\"}):\n",
    "    image = product.find('img').get('src')\n",
    "    title = product.find('h3', {'class': \"prdct-title\"}).text\n",
    "    data = [title, image]\n",
    "    write_json(data,file)\n",
    "\n",
    "def subproducts_scraper(file):\n",
    "  baseurl = 'https://www.bplmedicaltechnologies.com'\n",
    "  url1 = '/products/'\n",
    "  r = requests.get(url=baseurl+url1)\n",
    "  data = r.content\n",
    "  products = []\n",
    "  soup = BeautifulSoup(data, \"html.parser\")\n",
    "  for link in soup.findAll('a'):\n",
    "    rawlink = link.get('href')\n",
    "    if ('/products/' in rawlink):\n",
    "      rawlink = rawlink.split('/')[2]\n",
    "      if rawlink != '':\n",
    "        rawlink = url1+rawlink+'/'\n",
    "        if (rawlink not in products):\n",
    "          products.append(rawlink)\n",
    "\n",
    "  for i in range(len(products)):\n",
    "    subproducts = []\n",
    "    imagegallery = []\n",
    "    featuresls = []\n",
    "    namels = []\n",
    "    url2 = baseurl+products[i]\n",
    "    r = requests.get(url=url2)\n",
    "    data = r.content\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    for link in soup.findAll('a'):\n",
    "      rawlink = link.get('href')\n",
    "      if ('/product-details/' in rawlink):\n",
    "        if rawlink != '':\n",
    "          rawlink = baseurl+rawlink\n",
    "          if (rawlink not in subproducts):\n",
    "            subproducts.append(rawlink)\n",
    "\n",
    "    for j in range(len(subproducts)):\n",
    "      url = subproducts[j]\n",
    "      r = requests.get(url=url)\n",
    "      data = r.content\n",
    "      image = []\n",
    "      featureli = []\n",
    "      name = ''\n",
    "      cate = ''\n",
    "      soup = BeautifulSoup(data, \"html.parser\")\n",
    "      try:\n",
    "        souptumbnail = soup.find('div', {'class': \"pd-intr-image\"})\n",
    "        for thumbnail in souptumbnail.findAll('img'):\n",
    "          rawlink = thumbnail.get('src')\n",
    "          if 'https://www.bplmedicaltechnologies.com/' in rawlink:\n",
    "            image.append(rawlink)\n",
    "      except:\n",
    "        pass\n",
    "      try:\n",
    "        soupimg = soup.find('ul', {'id': \"gallery-flex\"})\n",
    "        for link in soupimg.findAll('img'):\n",
    "          rawlink = link.get('src')\n",
    "          if 'https://www.bplmedicaltechnologies.com/' in rawlink:\n",
    "            if rawlink not in image:\n",
    "              image.append(rawlink)\n",
    "      except:\n",
    "        pass\n",
    "      soupcat = soup.find('div', {'class': \"navigation\"})\n",
    "      try:\n",
    "        soupname = soup.find('div', {'class': \"new-layout-heading\"})\n",
    "        name = soupname.text.replace(\"\\n\", '')\n",
    "      except:\n",
    "        soupname = soup.find('div', {'class': \"navigation\"})\n",
    "        try:\n",
    "          name = soupcat.text.split(\"//\")[4].replace('\\r\\n', '')\n",
    "        except:\n",
    "          name = soupcat.text.split(\"//\")[2].replace('\\r\\n', '')\n",
    "\n",
    "      soupfeatures = soup.find('div', {'class': \"proct-feature-list\"})\n",
    "      for link in soupfeatures.find_all('li'):\n",
    "        feature = link.text\n",
    "        feature = re.sub('[^a-zA-Z0-9 : - /\\.]', '', feature)\n",
    "        featureli.append(feature)\n",
    "      cate = soupcat.text.split(\"//\")[1]\n",
    "      name = re.sub(' +', ' ', name)\n",
    "      cate = re.sub(' +', ' ', cate).replace('\\r\\n', '').strip()\n",
    "      cate = cate.replace(' ', '')\n",
    "      imagegallery.append(image)\n",
    "      featuresls.append(featureli)\n",
    "      namels.append(name)\n",
    "      j += 1\n",
    "    data = [[a, b, c] for a, b, c in zip(imagegallery, namels, featuresls)]\n",
    "    data = {cate: data}\n",
    "    write_json(data, file)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  newfile(\"subproducts.json\")\n",
    "  subproducts_scraper(\"subproducts.json\")\n",
    "  newfile(\"products.json\")\n",
    "  products_scraper(\"products.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa87ed25ce6ee8d1d803b1dff6e0813bead6bb663fd700c98d4a0f1c53df08b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
